{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collaborative Filtering ALS Recommender System using Spark MLlib adapted from\n",
    "the Spark Summit 2014 Recommender System training example.\n",
    "Supervisor: Dr. Magdalini Eirinaki\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.ml.recommendation import ALS as mlals\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling spark session to register application\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Recom\") \\\n",
    "    .config(\"spark.recom.demo\", \"1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading and Parsing Dataset\n",
    "    Each line in the ratings dataset (ratings.csv) is formatted as:\n",
    "         userId,movieId,rating,timestamp\n",
    "    Each line in the movies (movies.csv) dataset is formatted as:\n",
    "        movieId,title,genres\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "# Load ratings\n",
    "ratings_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|   196|    242|     3|\n",
      "|   186|    302|     3|\n",
      "|    22|    377|     1|\n",
      "|   244|     51|     2|\n",
      "|   166|    346|     1|\n",
      "|   298|    474|     4|\n",
      "|   115|    265|     2|\n",
      "|   253|    465|     5|\n",
      "|   305|    451|     3|\n",
      "|     6|     86|     3|\n",
      "|    62|    257|     2|\n",
      "|   286|   1014|     5|\n",
      "|   200|    222|     5|\n",
      "|   210|     40|     3|\n",
      "|   224|     29|     3|\n",
      "|   303|    785|     3|\n",
      "|   122|    387|     5|\n",
      "|   194|    274|     2|\n",
      "|   291|   1042|     4|\n",
      "|   234|   1184|     2|\n",
      "|   119|    392|     4|\n",
      "|   167|    486|     4|\n",
      "|   299|    144|     4|\n",
      "|   291|    118|     2|\n",
      "|   308|      1|     4|\n",
      "|    95|    546|     2|\n",
      "|    38|     95|     5|\n",
      "|   102|    768|     2|\n",
      "|    63|    277|     4|\n",
      "|   160|    234|     5|\n",
      "+------+-------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For the simplicity of this tutorial\n",
    "    For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). \n",
    "    We drop the timestamp because we do not need it for this recommender.\n",
    "\"\"\"\n",
    "\n",
    "#ratings_df = ratings_df.drop('timestamp')\n",
    "ratings_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies\n",
    "movies_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|   196|    242|     3|\n",
      "|   186|    302|     3|\n",
      "|    22|    377|     1|\n",
      "|   244|     51|     2|\n",
      "|   166|    346|     1|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|   196|    242|     3|\n",
      "|   186|    302|     3|\n",
      "|    22|    377|     1|\n",
      "|   244|     51|     2|\n",
      "|   166|    346|     1|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each line in the movies dataset, we create a tuple of (MovieID, Title). \n",
    "    We drop the genres because we do not use them for this recommender.\n",
    "\"\"\"\n",
    "movies_df = movies_df.drop('genres')\n",
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to determine the best ALS parameters, we will use the small dataset. \n",
    "We need first to split it into train, validation, and test datasets.\n",
    "\"\"\"\n",
    "(trainingData,validationData,testData) = ratings_df.randomSplit([0.6,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test and validation set. They should not have ratings\n",
    "\n",
    "validation_for_predict = validationData.select('userId','movieId')\n",
    "test_for_predict = testData.select('userId','movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by \n",
    "using Alternating Least Squares. The implementation in MLlib has the following parameters:\n",
    "\n",
    "    1. numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "    2. rank is the number of latent factors in the model.\n",
    "    3. iterations is the number of iterations to run.\n",
    "    4. lambda specifies the regularization parameter in ALS.\n",
    "    5. implicitPrefs specifies whether to use the explicit \n",
    "        feedback ALS variant or one adapted for implicit feedback data.\n",
    "    6. alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline \n",
    "        confidence in preference observations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed = 5 #Random seed for initial matrix factorization model. A value of None will use system time as the seed.\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1 #run for different lambdas - e.g. 0.01\n",
    "ranks = [4, 8, 12] #number of features\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.9471393221571547\n",
      "For rank 8 the RMSE is 0.9500848132443772\n",
      "For rank 12 the RMSE is 0.9531320554499474\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "# Let us traing our dataset and check the best rank with lowest RMSE\n",
    "# predictAll method of the ALS takes only RDD format and hence we need to convert our dataframe into RDD\n",
    "# df.rdd will automatically converts Dataframe into RDD\n",
    "\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainingData, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validationData.rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) # RMSE Error\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.944318387099151\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark will soon deprecate MLLIb package. \n",
    "They are focusing more on ML packages with standard machine learning implementation\n",
    "Let's see that package also\n",
    "\"\"\"\n",
    "als =  mlals(maxIter=iterations,rank=4,seed=seed,regParam=regularization_parameter, userCol=\"userId\", itemCol=\"movieId\",ratingCol=\"rating\")\n",
    "modelML = als.fit(trainingData)\n",
    "pred = modelML.transform(validationData)\n",
    "pred = pred.where(pred['prediction'] != 'NaN')\n",
    "    \n",
    "# Evaluate the model by computing RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "\n",
    "print ('RMSE is %s' % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take test dataset and get ratings\n",
    "predictions_test = model.predictAll(test_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((537, 1084), 2.6321352727544367),\n",
       " ((195, 1084), 3.589950939534698),\n",
       " ((655, 1084), 2.6643222707531677)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## visualize preditions, here third element is predictions generated by ALS Model\n",
    "predictions_test.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's start recommending movies.\n",
    "I have written a method to call recommendations for a perticular user from test data\n",
    "\n",
    "TODO: You need to execute one more step before calling getRecommendations, \n",
    "      Think about that step. If you go through the seps below, you will realize it soon.\n",
    "\"\"\"\n",
    "def getRecommendations(user,testDf,trainDf,model):\n",
    "    # get all user and his/her rated movies\n",
    "    userDf = testDf.filter(testDf.userId == user)\n",
    "    # filter movies from main set which have not been rated by selected user\n",
    "    # and pass it to model we sreated above\n",
    "    mov = trainDf.select('movieId').subtract(userDf.select('movieId'))\n",
    "    \n",
    "    # Again we need to covert our dataframe into RDD\n",
    "    pred_rat = model.predictAll(mov.rdd.map(lambda x: (user, x[0]))).collect()\n",
    "    \n",
    "    # Get the top recommendations\n",
    "    recommendations = sorted(pred_rat, key=lambda x: x[2], reverse=True)[:5]\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended for:336\n"
     ]
    }
   ],
   "source": [
    "# Assign user id for which we need recommendations\n",
    "user = 336\n",
    "\n",
    "# Call getRecommendations method\n",
    "derived_rec = getRecommendations(user,testData,trainingData,model)\n",
    "\n",
    "print (\"Movies recommended for:%d\" % user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=336, product=1643, rating=4.993348613988031),\n",
       " Rating(user=336, product=1240, rating=4.441544940238194),\n",
       " Rating(user=336, product=1589, rating=4.426185569048147),\n",
       " Rating(user=336, product=156, rating=4.34982412293569),\n",
       " Rating(user=336, product=1449, rating=4.343126036631487)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
